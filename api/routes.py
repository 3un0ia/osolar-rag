import json
from flask import Response
from flask import Blueprint, request, jsonify
from services.qa_chain import run_qa, vectordb, template
from services.llm_client import generate_response
import services.backend_client as backend_client
import re
CONTROL_CHAR_RE = re.compile(r'[\x00-\x1F]+')

bp = Blueprint("api", __name__)

@bp.route("/query", methods=["POST"])
def query():
    data = request.get_json()

    question = data.get("question","")
    top_k = data.get("top_k", 5)
    skip_llm = data.get("skip_llm", False)

    # 1) ê²€ìƒ‰
    answer, docs = run_qa(question)
    # # 2) í”„ë¡¬í”„íŠ¸
    # context = "\n\n".join(doc.page_content for doc, _ in docs_scores)
    # prompt  = template.format(
    #             context=context, 
    #             question=question, 
    #             user_profile=user_profile
    #     )
    # 3) LLM í˜¸ì¶œ
    if skip_llm:
        answer = None

    # 4) ê²°ê³¼ ë°˜í™˜
    return jsonify({
        "answer": answer,
        "documents": [
            {
                "id": d.metadata.get("id"), 
                "score": getattr(d, "score", None)
            }
            for d in docs
        ]
    })

@bp.route("/search_docs", methods=["POST"])
def search_docs():
    data    = request.get_json()

    query   = data.get("query", "")
    top_k   = data.get("top_k", 5)

    docs_scores = vectordb.similarity_search_with_score(query, k=top_k)
    
    out = []

    import re
    CONTROL_CHAR_RE = re.compile(r'[\x00-\x1F]+')

    for doc, score in docs_scores:
        text = CONTROL_CHAR_RE.sub('', doc.page_content)
        out.append({
            "id":           doc.metadata.get("id"),
            "score":        score,
            "metadata":     doc.metadata,
            "page_content": text
        })

    payload = out

    body = json.dumps(payload, ensure_ascii=False)
    return Response(body, content_type="application/json; charset=utf-8")



@bp.route("/prompt", methods=["POST"])
def prompt():
    data         = request.get_json() or {}

    docs         = data.get("documents", [])
    question     = data.get("question", "")
    user_profile = data.get("user_profile", {})

    # Context ì¡°ë¦½: front-endì—ì„œ ë³´ë‚´ì¤€ docs ë¦¬ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    context = "\n\n".join(d.get("page_content", "") for d in docs)

    # LangChain PromptTemplate ì— ì…ë ¥ ë³€ìˆ˜ë¡œ ë„˜ê²¨ì„œ ë¬¸ìì—´ ìƒì„±
    prompt_str = template.format(
        context=context,
        question=question,
        user_profile=user_profile
    )
    return jsonify({"prompt": prompt_str})


@bp.route("/llm", methods=["POST"])
def llm_only():
    data   = request.get_json() or {}
    prompt = data.get("prompt", "")

    # Bedrock í˜¸ì¶œ
    try:
        answer = generate_response(prompt)
    except Exception as e:
        print("ğŸ”´ LLM í˜¸ì¶œ ì˜¤ë¥˜:", e)
        return jsonify({"error": str(e)}), 400
    return jsonify({"response": answer})